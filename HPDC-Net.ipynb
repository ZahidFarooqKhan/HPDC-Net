{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🌿📱 HPDC-Net: Edge Device-Friendly CNN for Plant Leaf Disease Classification 🌾🧠\n",
        "\n",
        "Welcome to the **HPDC-Net** 👨‍🔬🧪  \n",
        "A **High-Performance Disease Classification Network (HPDC-Net)** designed for real-time, on-device plant disease detection using leaf images.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🌟 Key Features of HPDC-Net\n",
        "\n",
        "🔹 **Lightweight Architecture** – Perfect for edge deployment on mobile or IoT devices (Raspberry Pi, Jetson Nano).  \n",
        "🔹 **High Accuracy** – Designed for robust performance on small datasets of plant leaf images.  \n",
        "🔹 **Optimized Inference** – Fast and efficient with low memory and power usage.  \n",
        "🔹 **Smart Feature Extraction** – Includes novel modules for capturing disease-specific patterns in leaf texture and color.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WeO_ZKg9Z9kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📂 Dataset Loading and Preparation 🌿🧪\n",
        "\n",
        "In this section, we will load and prepare the **Plant Leaf Disease Dataset** for training and evaluation using **PyTorch**. The dataset includes multiple classes such as healthy and diseased leaves from various crops. 🍅🍇🌾\n"
      ],
      "metadata": {
        "id": "sAvuKnynaU-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfAsXDUX7wkV",
        "outputId": "dbcb9885-47a6-426a-b929-abe26d5f8989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-18 11:13:37--  https://data.mendeley.com/public-files/datasets/tywbtsjrjv/files/d5652a28-c1d8-4b76-97f3-72fb80f94efc/file_downloaded\n",
            "Resolving data.mendeley.com (data.mendeley.com)... 162.159.133.86, 162.159.130.86\n",
            "Connecting to data.mendeley.com (data.mendeley.com)|162.159.133.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/d29ed9b2-8a5d-4663-8a82-c9174f2c7066 [following]\n",
            "--2025-05-18 11:13:38--  https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/d29ed9b2-8a5d-4663-8a82-c9174f2c7066\n",
            "Resolving prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com)... 3.5.71.147, 52.92.19.186, 52.218.105.131, ...\n",
            "Connecting to prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com)|3.5.71.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 868032562 (828M) [application/zip]\n",
            "Saving to: ‘file_downloaded’\n",
            "\n",
            "file_downloaded     100%[===================>] 827.82M  18.4MB/s    in 50s     \n",
            "\n",
            "2025-05-18 11:14:28 (16.7 MB/s) - ‘file_downloaded’ saved [868032562/868032562]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://data.mendeley.com/public-files/datasets/tywbtsjrjv/files/d5652a28-c1d8-4b76-97f3-72fb80f94efc/file_downloaded\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-OPSTDOvM1Hg"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/file_downloaded\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxqKxcSWX6MU"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('tomato')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQjDqjIP9F_F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the directory containing the folders\n",
        "directory = \"/content/Plant_leave_diseases_dataset_without_augmentation\"\n",
        "\n",
        "# Get a list of all folders in the directory\n",
        "folders = [\n",
        "    f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))\n",
        "]\n",
        "\n",
        "# Iterate through the folders and delete those that do not contain \"tomato\" in their title\n",
        "for folder in folders:\n",
        "    if \"tomato\" not in folder.lower():\n",
        "        folder_path = os.path.join(directory, folder)\n",
        "        print(f\"Deleting folder: {folder_path}\")\n",
        "        shutil.rmtree(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UJ5DoRlNL6U"
      },
      "outputs": [],
      "source": [
        "#split dataset\n",
        "!pip install split_folders\n",
        "import splitfolders\n",
        "\n",
        "base_dir = '/content/Plant_leave_diseases_dataset_without_augmentation'\n",
        "splitfolders.ratio(base_dir, output='/content/tomato', seed = 1314, ratio = (.7, .2, .1))\n",
        "\n",
        "train_dir = os.path.join('/content/tomato', 'train')\n",
        "validation_dir = os.path.join('/content/tomato', 'val')\n",
        "test_dir = os.path.join('/content/tomato', 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAAhB2s57LQo",
        "outputId": "2e6e3faa-b129-4940-f811-cff3253621c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 10\n",
            "Class names: ['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Function to set seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# Set seed\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "# Define the root directory of the dataset\n",
        "data_dir = \"/content/tomato\"\n",
        "\n",
        "# Define transformations for training, validation, and testing sets\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "valid_test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load datasets with ImageFolder\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"train\"), transform=train_transforms\n",
        ")\n",
        "valid_dataset = datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"val\"), transform=valid_test_transforms\n",
        ")\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"test\"), transform=valid_test_transforms\n",
        ")\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print the number of classes\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Class names: {train_dataset.classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVKOOAt97TOP",
        "outputId": "8683f609-697f-4d29-a898-fb189c0a72d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ],
      "source": [
        "!pip install thop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🌱🧠 **HPDCNet** is a lightweight convolutional neural network that combines depthwise adaptive residuals, hybrid pooling, and conditional feature enhancement. Designed for efficient and robust visual recognition, it fuses multi-scale features with recalibrated attention for superior performance.\n",
        "### 🔬 Potential Applications:\n",
        "\n",
        "✅ Smart Farming  \n",
        "✅ Mobile Disease Detection Apps  \n",
        "✅ Precision Agriculture  \n",
        "✅ Drone Crop Monitoring  \n"
      ],
      "metadata": {
        "id": "GwCkFl3Cc8lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Block Definitions ---\n",
        "\n",
        "class DepthwiseAdaptiveResidualBlock(nn.Module):\n",
        "    \"\"\"Depthwise separable convolution with adaptive residual connections.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(DepthwiseAdaptiveResidualBlock, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(\n",
        "            in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels\n",
        "        )\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x + identity)\n",
        "\n",
        "\n",
        "class FeatureRecalibrationAttention(nn.Module):\n",
        "    \"\"\"Recalibrates feature importance through channel-wise attention.\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FeatureRecalibrationAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention = self.sigmoid(self.conv1(x))\n",
        "        return x * attention\n",
        "\n",
        "\n",
        "class MultiScaleFusionBlock(nn.Module):\n",
        "    \"\"\"Fuses multi-scale features for enhanced representation.\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(MultiScaleFusionBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels, in_channels, kernel_size=5, padding=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x3 = self.conv3(x)\n",
        "        x5 = self.conv5(x)\n",
        "        return x1 + x3 + x5\n",
        "\n",
        "\n",
        "class HybridPoolingLayer(nn.Module):\n",
        "    \"\"\"Combines max and average pooling for robust feature extraction.\"\"\"\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        super(HybridPoolingLayer, self).__init__()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size, stride)\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        max_pooled = self.max_pool(x)\n",
        "        avg_pooled = self.avg_pool(x)\n",
        "        return (max_pooled + avg_pooled) / 2\n",
        "\n",
        "\n",
        "class ConditionalFeatureEnhancement(nn.Module):\n",
        "    \"\"\"Enhances features conditionally based on global context.\"\"\"\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ConditionalFeatureEnhancement, self).__init__()\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        weights = self.global_avg_pool(x).view(b, c)\n",
        "        weights = F.relu(self.fc1(weights))\n",
        "        weights = torch.sigmoid(self.fc2(weights)).view(b, c, 1, 1)\n",
        "        return x * weights\n",
        "\n",
        "\n",
        "# --- HPDC-Net ---\n",
        "\n",
        "class HPDCNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(HPDCNet, self).__init__()\n",
        "        # Stage 1\n",
        "        self.layer1 = DepthwiseAdaptiveResidualBlock(3, 16)\n",
        "        self.attn1 = FeatureRecalibrationAttention(16)\n",
        "        self.pool1 = HybridPoolingLayer()\n",
        "\n",
        "        # Stage 2\n",
        "        self.layer2 = DepthwiseAdaptiveResidualBlock(16, 32)\n",
        "        self.attn2 = FeatureRecalibrationAttention(32)\n",
        "        self.pool2 = HybridPoolingLayer()\n",
        "\n",
        "        # Stage 3\n",
        "        self.layer3 = DepthwiseAdaptiveResidualBlock(32, 64)\n",
        "        self.fusion3 = MultiScaleFusionBlock(64)\n",
        "        self.attn3 = FeatureRecalibrationAttention(64)\n",
        "        self.pool3 = HybridPoolingLayer()\n",
        "\n",
        "        # Stage 4\n",
        "        self.layer4 = DepthwiseAdaptiveResidualBlock(64, 128)\n",
        "        self.fusion4 = MultiScaleFusionBlock(128)\n",
        "        self.attn4 = FeatureRecalibrationAttention(128)\n",
        "\n",
        "        # Classifier\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stage 1\n",
        "        x = self.layer1(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Stage 2\n",
        "        x = self.layer2(x)\n",
        "        x = self.attn2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Stage 3\n",
        "        x = self.layer3(x)\n",
        "        x = self.fusion3(x)\n",
        "        x = self.attn3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Stage 4\n",
        "        x = self.layer4(x)\n",
        "        x = self.fusion4(x)\n",
        "        x = self.attn4(x)\n",
        "\n",
        "        # Classifier\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model Summary\n",
        "if __name__ == \"__main__\":\n",
        "    model = HPDCNet(num_classes=3)\n",
        "    sample_input = torch.randn(1, 3, 224, 224)\n",
        "    output = model(sample_input)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Model Summary:\")\n",
        "    from thop import profile\n",
        "\n",
        "    flops, params = profile(model, inputs=(sample_input,))\n",
        "    print(f\"GFLOPs: {flops / 1e9:.2f}\")\n",
        "    print(f\"Number of parameters (in millions): {params / 1e6:.2f}\")"
      ],
      "metadata": {
        "id": "YgVrJxlFclew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e2rR8T1-opR"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm torchmetrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🏋️ Model Training Procedure\n",
        "\n",
        "The `HPDCNet` model was trained for 200 epochs using the Adam optimizer with a learning rate of `0.001` and CrossEntropyLoss.  \n",
        "Training and validation were conducted on a multi-class classification task with 10 classes.\n",
        "\n",
        "**Training Pipeline:**\n",
        "- Device: GPU\n",
        "- Data: Loaded via `train_loader` and `valid_loader`\n",
        "- Metrics: Accuracy, Precision, Recall, and F1 Score (macro average using TorchMetrics)\n",
        "- Best model checkpoint saved based on highest validation accuracy\n"
      ],
      "metadata": {
        "id": "fJ8kvo34dlB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VYRI1oiC92bI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import torchmetrics\n",
        "\n",
        "# Assume the dataset and data loaders are already defined\n",
        "# Example:\n",
        "# train_loader = ...\n",
        "# valid_loader = ...\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model\n",
        "num_classes = 10  # Example number of classes\n",
        "model = HPDCNet(num_classes).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and validation loop\n",
        "num_epochs = 200\n",
        "\n",
        "# Metrics\n",
        "train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(\n",
        "    device\n",
        ")\n",
        "train_precision = torchmetrics.Precision(\n",
        "    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
        ").to(device)\n",
        "train_recall = torchmetrics.Recall(\n",
        "    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
        ").to(device)\n",
        "train_f1_score = torchmetrics.F1Score(\n",
        "    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
        ").to(device)\n",
        "\n",
        "valid_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(\n",
        "    device\n",
        ")\n",
        "valid_precision = torchmetrics.Precision(\n",
        "    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
        ").to(device)\n",
        "valid_recall = torchmetrics.Recall(\n",
        "    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
        ").to(device)\n",
        "valid_f1_score = torchmetrics.F1Score(\n",
        "    task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
        ").to(device)\n",
        "\n",
        "# Variable to track the best validation accuracy\n",
        "best_val_accuracy = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy.reset()\n",
        "    train_precision.reset()\n",
        "    train_recall.reset()\n",
        "    train_f1_score.reset()\n",
        "\n",
        "    for inputs, labels in tqdm(\n",
        "        train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"\n",
        "    ):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_accuracy.update(outputs, labels)\n",
        "        train_precision.update(outputs, labels)\n",
        "        train_recall.update(outputs, labels)\n",
        "        train_f1_score.update(outputs, labels)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy_value = train_accuracy.compute().item()\n",
        "    train_precision_value = train_precision.compute().item()\n",
        "    train_recall_value = train_recall.compute().item()\n",
        "    train_f1_value = train_f1_score.compute().item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy.reset()\n",
        "    valid_precision.reset()\n",
        "    valid_recall.reset()\n",
        "    valid_f1_score.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(\n",
        "            valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"\n",
        "        ):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_accuracy.update(outputs, labels)\n",
        "            valid_precision.update(outputs, labels)\n",
        "            valid_recall.update(outputs, labels)\n",
        "            valid_f1_score.update(outputs, labels)\n",
        "\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_accuracy_value = valid_accuracy.compute().item()\n",
        "    valid_precision_value = valid_precision.compute().item()\n",
        "    valid_recall_value = valid_recall.compute().item()\n",
        "    valid_f1_value = valid_f1_score.compute().item()\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if valid_accuracy_value > best_val_accuracy:\n",
        "        best_val_accuracy = valid_accuracy_value\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\n",
        "            f\"New best model saved at epoch {epoch+1} with validation accuracy: {best_val_accuracy:.4f}\"\n",
        "        )\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Training Loss: {train_loss:.4f} | Validation Loss: {valid_loss:.4f}\")\n",
        "    print(\n",
        "        f\"Training Accuracy: {train_accuracy_value:.4f} | Validation Accuracy: {valid_accuracy_value:.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Training Precision: {train_precision_value:.4f} | Validation Precision: {valid_precision_value:.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Training Recall: {train_recall_value:.4f} | Validation Recall: {valid_recall_value:.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Training F1 Score: {train_f1_value:.4f} | Validation F1 Score: {valid_f1_value:.4f}\"\n",
        "    )\n",
        "\n",
        "print(f\"Best validation accuracy: {best_val_accuracy:.4f} at epoch {best_epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🏋️ Model Training on CPU\n"
      ],
      "metadata": {
        "id": "1bOp9CZzeTK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTtw9E-GgDKB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchmetrics\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "# Define the test dataset with ImageFolder to get class names\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"test\"), transform=valid_test_transforms\n",
        ")\n",
        "class_names = test_dataset.classes  # Extract class names from the folder names\n",
        "\n",
        "# Force CPU usage\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load the trained model\n",
        "num_classes = len(class_names)\n",
        "model = HPDCNet(num_classes).to(device)\n",
        "model_path = \"/content/best_model.pth\"\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Define the test data loader\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Metrics\n",
        "test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "test_precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
        "test_recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
        "test_f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
        "test_confusion_matrix = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "\n",
        "# Function to perform inference and calculate metrics\n",
        "def perform_inference(device):\n",
        "    model.to(device)\n",
        "    test_accuracy.to(device)\n",
        "    test_precision.to(device)\n",
        "    test_recall.to(device)\n",
        "    test_f1_score.to(device)\n",
        "    test_confusion_matrix.to(device)\n",
        "\n",
        "    inference_times = []\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for i in range(5):\n",
        "        test_accuracy.reset()\n",
        "        test_precision.reset()\n",
        "        test_recall.reset()\n",
        "        test_f1_score.reset()\n",
        "        test_confusion_matrix.reset()\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                test_accuracy.update(outputs, labels)\n",
        "                test_precision.update(outputs, labels)\n",
        "                test_recall.update(outputs, labels)\n",
        "                test_f1_score.update(outputs, labels)\n",
        "                test_confusion_matrix.update(predicted, labels)\n",
        "\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        end_time = time.time()\n",
        "        inference_times.append((end_time - start_time) * 1000)  # ms\n",
        "\n",
        "    avg_inference_time = np.mean(inference_times) / len(test_loader.dataset)\n",
        "    avg_accuracy = test_accuracy.compute().item()\n",
        "    avg_precision = test_precision.compute().item()\n",
        "    avg_recall = test_recall.compute().item()\n",
        "    avg_f1 = test_f1_score.compute().item()\n",
        "    cm = test_confusion_matrix.compute().cpu().numpy()\n",
        "\n",
        "    return avg_accuracy, avg_precision, avg_recall, avg_f1, avg_inference_time, cm, all_labels, all_predictions\n",
        "\n",
        "# Perform inference on CPU\n",
        "(\n",
        "    avg_accuracy_cpu,\n",
        "    avg_precision_cpu,\n",
        "    avg_recall_cpu,\n",
        "    avg_f1_cpu,\n",
        "    avg_inference_time_cpu,\n",
        "    cm_cpu,\n",
        "    labels_cpu,\n",
        "    preds_cpu,\n",
        ") = perform_inference(device)\n",
        "\n",
        "fps_cpu = 1000 / avg_inference_time_cpu\n",
        "print(f\"CPU Average Test Accuracy: {avg_accuracy_cpu:.4f}\")\n",
        "print(f\"CPU Average Test Precision: {avg_precision_cpu:.4f}\")\n",
        "print(f\"CPU Average Test Recall: {avg_recall_cpu:.4f}\")\n",
        "print(f\"CPU Average Test F1 Score: {avg_f1_cpu:.4f}\")\n",
        "print(f\"CPU Average Inference Time per Image: {avg_inference_time_cpu:.6f} ms\")\n",
        "print(f\"CPU Frames per Second: {fps_cpu:.2f}\")\n",
        "\n",
        "# Display confusion matrix\n",
        "def plot_confusion_matrix(cm, class_names, title):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "    )\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(cm_cpu, class_names, \"CPU Confusion Matrix\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Visualizing Model Predictions on Sample Test Images\n",
        "\n",
        "To better understand how the model performs on real data, we selected a set of 20 representative test images—two from each class (when available).\n",
        "\n",
        "For each image, we performed forward inference and compared the **predicted label** against the **true label**:\n",
        "\n",
        "- ✅ Correct predictions are shown with **green titles**\n",
        "- ❌ Incorrect predictions are highlighted in **red**\n",
        "\n",
        "The images are displayed in a grid layout for quick inspection. This visualization helps:\n",
        "\n",
        "- Evaluate **class-wise performance**\n",
        "- Identify **failure patterns**\n",
        "- Visually inspect prediction **confidence and mistakes**\n"
      ],
      "metadata": {
        "id": "VBrZHPXPfGY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilxFWSHJMQeo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "# Ensure device is correctly set\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Collect images by class\n",
        "def collect_images_by_class(test_loader):\n",
        "    class_indices = {i: [] for i in range(num_classes)}\n",
        "    all_labels = []\n",
        "    all_images = []\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        all_images.extend(inputs)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    all_images = torch.stack(all_images).to(device)\n",
        "    all_labels = torch.tensor(all_labels).to(device)\n",
        "\n",
        "    for idx, label in enumerate(all_labels):\n",
        "        class_indices[label.item()].append(idx)\n",
        "\n",
        "    return class_indices, all_images, all_labels\n",
        "\n",
        "\n",
        "# Get images by class\n",
        "class_indices, all_images, all_labels = collect_images_by_class(test_loader)\n",
        "\n",
        "# Select two images per class\n",
        "selected_indices = []\n",
        "for indices in class_indices.values():\n",
        "    if len(indices) >= 2:\n",
        "        selected_indices.extend(random.sample(indices, 2))\n",
        "    else:\n",
        "        selected_indices.extend(indices)\n",
        "\n",
        "# Ensure we have 20 images\n",
        "if len(selected_indices) < 20:\n",
        "    remaining_indices = [i for i in range(len(all_labels)) if i not in selected_indices]\n",
        "    additional_indices = random.sample(remaining_indices, 20 - len(selected_indices))\n",
        "    selected_indices.extend(additional_indices)\n",
        "\n",
        "# Gather the selected images and labels\n",
        "selected_images = all_images[selected_indices]\n",
        "selected_labels = all_labels[selected_indices]\n",
        "\n",
        "# Perform inference on the selected images\n",
        "with torch.no_grad():\n",
        "    outputs = model(selected_images)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "# Plot images\n",
        "def plot_image_with_labels(image, predicted_label, true_label, ax, class_names):\n",
        "    image = inv_transform(image.cpu())\n",
        "    ax.imshow(image)\n",
        "    if predicted_label == true_label:\n",
        "        ax.set_title(\n",
        "            f\"Pred: {class_names[predicted_label]}, True: {class_names[true_label]}\",\n",
        "            color=\"green\",\n",
        "        )\n",
        "    else:\n",
        "        ax.set_title(\n",
        "            f\"Pred: {class_names[predicted_label]}, True: {class_names[true_label]}\",\n",
        "            color=\"red\",\n",
        "        )\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "\n",
        "# Inverse transform for displaying images\n",
        "inv_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Normalize(\n",
        "            mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
        "            std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
        "        ),\n",
        "        transforms.ToPILImage(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Plotting the selected images\n",
        "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
        "\n",
        "for idx, ax in zip(range(len(selected_images)), axes.flat):\n",
        "    plot_image_with_labels(\n",
        "        selected_images[idx],\n",
        "        predicted_labels[idx].item(),\n",
        "        selected_labels[idx].item(),\n",
        "        ax,\n",
        "        class_names,\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lCI_YyWGbCIE",
        "outputId": "56767ae2-0765-4d18-d88d-3c850fc3ad6e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_08bb1588-9137-4e1f-b5cb-2354504bdd7b\", \"best_model.zip\", 2094176)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Path to your model file\n",
        "model_file_path = \"/content/best_model.pth\"\n",
        "\n",
        "# Path to save the zip file\n",
        "zip_file_path = \"/content/best_model.zip\"\n",
        "\n",
        "# Create a zip file and add the model file to it\n",
        "with zipfile.ZipFile(zip_file_path, \"w\") as zipf:\n",
        "    zipf.write(model_file_path, os.path.basename(model_file_path))\n",
        "\n",
        "# Download the zip file to your local machine\n",
        "files.download(zip_file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}